{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-class SVM\n",
    "클래스가 2개 이상인 경우를 다중 클래스 분류(Multi-Class Classification)라 한다. 다중 클래스 분류 문제는 다음과 같이 여러개의 이진 클래스 분류(Binary Class Classification) 문제로 변환하여 해결한다.\n",
    "<img src=\"https://www.researchgate.net/profile/Anthony_Fleury/publication/220098164/figure/fig2/AS:305725600485382@1449902071844/Figure-2-Illustration-of-the-SVM-principle-and-of-the-one-versus-one-multiclass.png\" width=\"600\" height=\"600\" />\n",
    "<br>\n",
    "#### OvR (One-vs-the-Rest)\n",
    "K 개의 타겟 클래스가 존재하는 경우,\n",
    "각각의 클래스에 대해 표본이 속하는지 속하지 않는지의 이진 클래스 분류 문제를 풀고\n",
    "판결 기준값이 가장 큰 클래스를 선택 OneVsRestClassifier 클래스\n",
    "<img src=\"https://pythonprogramming.net/static/images/machine-learning/one-vs-rest-svm.png\" width=\"300\" height=\"300\" />\n",
    "<br>\n",
    "#### OvO (One-Vs-One)\n",
    "K 개의 타겟 클래스가 존재하는 경우,\n",
    "이 중 2개의 클래스 조합을 선택하여  K(K−1)/2K(K−1)/2 개의 이진 클래스 분류 문제를 풀고\n",
    "투표를 통해 가장 많은 표를 얻은 클래스를 선택\n",
    "실제로는 정규화된 판결 기준값을 이용 OneVsOneClassifier 클래스\n",
    "<img src=\"https://pythonprogramming.net/static/images/machine-learning/one-vs-one-svm.png\" width=\"300\" height=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3가지의 의도로 Text를 분류해보자 (피자주문, 호텔예약, 여행정보)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=15, size=50, alpha=0.025)\n",
      "['에', '판교', '오늘', '해', '날짜', '피자', '주문', '줘', '호텔', '예약', '줄레', '모래', '여행', '정보', '알려줘']\n",
      "['판교', '에', '오늘', '피자', '주문', '해', '줘', '#']\n",
      "['오늘', '날짜', '에', '호텔', '예약', '해', '줄레', '#']\n",
      "['모래', '날짜', '에', '판교', '여행', '정보', '알려줘', '#']\n"
     ]
    }
   ],
   "source": [
    "from eunjeon import Mecab\n",
    "import numpy as np\n",
    "\n",
    "train_data_list =  {\n",
    "                    'encode' : ['판교에 오늘 피자 주문해줘','오늘 날짜에 호텔 예약 해줄레','모래 날짜에 판교 여행 정보 알려줘'],\n",
    "                    'decode' : ['0','1','2']\n",
    "                   }\n",
    "\n",
    "embed_type = 'onehot'\n",
    "encode_length = 8 #문장의 최대 길이 나머지는 Padding로 채움\n",
    "vector_size = 50\n",
    "label_size = 3 #Label의 수\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "def train_vector_model(str_buf):\n",
    "    mecab = Mecab()\n",
    "    str_buf = train_data_list['encode']\n",
    "    #mecab로 POS Tagging\n",
    "    pos1 = mecab.pos(''.join(str_buf))\n",
    "    #문장별로 list로 나눔 마침표등이 존재시 줄바꾸기 (문장이길경우)\n",
    "    pos2 = ' '.join(list(map(lambda x : '\\n' if x[1] in ['SF'] else x[0], pos1))).split('\\n')\n",
    "    #단어구성을 위한 형태소단위 문장 쪼개기 \n",
    "    morphs = list(map(lambda x : mecab.morphs(x) , pos2))\n",
    "    model = word2vec.Word2Vec(size=vector_size, window=2, min_count=1)\n",
    "    model.build_vocab(morphs)\n",
    "    model.train(morphs, epochs=model.epochs, total_examples=model.corpus_count)\n",
    "    return model\n",
    "# W2V Vector 모델 생성\n",
    "model = train_vector_model(train_data_list)\n",
    "print(model)\n",
    "print(model.wv.index2word) #Word List를 구함\n",
    "\n",
    "def onehot_vectorize(bucket, x):\n",
    "    #W2V의 Vocab의 해당 index값을 onehot으로 만듬\n",
    "    np.put(bucket, model.wv.index2word.index(x),1)\n",
    "    return bucket\n",
    "\n",
    "def embed(data) : \n",
    "    mecab = Mecab()\n",
    "    inputs = []\n",
    "#   labels = []\n",
    "    for encode_raw in data['encode'] : \n",
    "        encode_raw = mecab.morphs(encode_raw)\n",
    "        encode_raw = list(map(lambda x : encode_raw[x] if x < len(encode_raw) else '#', range(encode_length)))\n",
    "        if(embed_type == 'onehot') :\n",
    "            bucket = np.zeros(vector_size, dtype=float).copy()\n",
    "            input = np.array(list(map(lambda x : onehot_vectorize(bucket, x) if x in model.wv.index2word else np.zeros(vector_size,dtype=float) , encode_raw)))\n",
    "        inputs.append(input.flatten())\n",
    "        print(encode_raw)\n",
    "#     for decode_raw in data['decode']: \n",
    "#         label = np.zeros(label_size, dtype=float)\n",
    "#         np.put(label, decode_raw, 1)\n",
    "#         labels.append(label)\n",
    "    return inputs #labels\n",
    "\n",
    "#X, y = embed(train_data_list) #Encode와 Decode Data를 X와 y값에 Vector값을 담음\n",
    "X = embed(train_data_list) #Encode와 Decode Data를 X와 y값에 Vector값을 담음\n",
    "y = train_data_list['decode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0']\n",
      "['1']\n",
      "['2']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(X, y) \n",
    "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "#print(clf.predict([[2, 2]]))\n",
    "print(clf.predict([X[0]]))\n",
    "print(clf.predict([X[1]]))\n",
    "print(clf.predict([X[2]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
